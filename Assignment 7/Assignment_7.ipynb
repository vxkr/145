{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FWVlmdEtAFg"
      },
      "source": [
        "# Assignment 7: t-SNE for Dimensionality Reduction and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ9bmxxfC0j1",
        "colab_type": "text"
      },
      "source": [
        "## 1. PCA vs t-SNE\n",
        "\n",
        "Previously we looked at PCA as a method for dimensionality reduction by transforming data using a basis of the direction of maximum variation. Here we'll compare the two methods using the out of the box methods from scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MFoh_6BU29bH"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5Jcf4dN1yny",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wfbf6mFi2e2t",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ej11C7an2_bx"
      },
      "source": [
        "Some Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lx9ovFYfsvIu",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.utils import shuffle\n",
        "digits = load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdoukUqS8c27",
        "colab": {}
      },
      "source": [
        "X = digits.data\n",
        "y = digits.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWAMlp-x8iFC",
        "colab": {}
      },
      "source": [
        "# Scale X to be between 0 and 1 (avoid magic numbers - define global constants and use them!)\n",
        "\n",
        "# TODO: Scale X to 0 and 1\n",
        "X = ... \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G6lhko3b14wv",
        "colab": {}
      },
      "source": [
        "# Shuffle data\n",
        "\n",
        "X, y = shuffle(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPX_XrEH4uxf",
        "colab": {}
      },
      "source": [
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yegKd-g42hWi",
        "colab": {}
      },
      "source": [
        "# Look at the data we're working with\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(y[i])    \n",
        "    # TODO: Plot X[i]\n",
        "    ...\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o7N0gIxo4rPm"
      },
      "source": [
        "As you can see from the above code, our input data has 64 features, each resembling a pixel in each image. We want to reduce the dimensionality of our input using both PCA and t-SNE to visualize all the data points on one chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB5qbJbVC42y",
        "colab_type": "text"
      },
      "source": [
        "### 1.a) PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0DTpzL6n4WAw",
        "colab": {}
      },
      "source": [
        "# Carry out PCA on X\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "# TODO: Run PCA on X to get the first 2 principal components\n",
        "X_pca = ...\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h0sNPCxX6hwg",
        "colab": {}
      },
      "source": [
        "# Visualize X_pca\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(\n",
        "    x=X_pca[:,0], y=X_pca[:,1],\n",
        "    hue=y,\n",
        "    palette=sns.color_palette(\"hls\", 10),\n",
        "    legend=\"full\",\n",
        "    alpha=0.75)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShvSoMEPDCTW",
        "colab_type": "text"
      },
      "source": [
        "### 1.b) t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqSKJPV97Er7",
        "colab": {}
      },
      "source": [
        "# Carry out t-SNE on X\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "# TODO: Run t-SNE on X\n",
        "X_tsne = ...\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q2A7guY177WC",
        "colab": {}
      },
      "source": [
        "# Visualize X_tsne\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(\n",
        "    x=X_tsne[:,0], y=X_tsne[:,1],\n",
        "    hue=y,\n",
        "    palette=sns.color_palette(\"hls\", 10),\n",
        "    legend=\"full\",\n",
        "    alpha=0.75\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j06S0gPS92MF"
      },
      "source": [
        "## 2. Implementing t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEk6cgwBn4f",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, we follow the implementation of t-SNE directly from the 2008 paper by Maaten and Hinton. It builds upon SNE (Stochastic Neighbor Embedding) and \"reduces the tendency to crowd points together in the center\n",
        "of the map.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuaXkMCYBn4f",
        "colab_type": "text"
      },
      "source": [
        "#### Intro/Definitions\n",
        "\n",
        "The goal of t-SNE is to define a location $y_i$ in low-dimension space for high-dimensional points $x_i$. Each point $x_i$ has a probability distribution associated with it of picking another point $x_j$ as its neighbor, defined as $p_{j|i}$ in equation 1 of the paper. $P_i$ is defined as the distribution of other high-dimensional points given $x_i$. Analagously for the low-dimensional points (which we have yet to find), $Q_i$ is defined as the distribution of other low-dimensional points given $y_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XqkcJb4Bn4g",
        "colab_type": "text"
      },
      "source": [
        "#### Cost function description\n",
        "\n",
        "In SNE, the cost function that we're trying to minimize is the \"difference\" between $P_i$ and $Q_i$ (we want them to be similar). How do we measure difference for probability distributions? The Kullback-Leibler divergence is a measure of dissimilarity between two distributions $P$ and $Q$, defined as $$ \\sum_i \\left( P(i) \\cdot log \\frac{P(i)}{Q(i)} \\right) $$ for all values $i$ that $P$ and $Q$ take on. You can see how if $P = Q$ for all $i$ (same distribution), the log term will always be $\\log 1 = 0$, and so the KLD will be 0 (no dissimilarity). \n",
        "\n",
        "As they state in the paper, \"In particular, there\n",
        "is a large cost for using widely separated map points to represent nearby datapoints (i.e., for using a small $q_{j|i}$ to model a large $p_{j|i}$), but there is only a small cost for using nearby map points to\n",
        "represent widely separated datapoints.\" You can see that is true - if for some pair of points the $q$ is big and the $p$ is not, the term $$p\\cdot log \\frac{p}{q}$$ will be small! That will mean that it can mistraslate to low dimension sneakily without seeming like the cost is going up. That is one of the motivations behind t-SNE over SNE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tDWo1SM68GxL",
        "colab": {}
      },
      "source": [
        "def Hbeta(D=np.array([]), beta=1.0):\n",
        "    \"\"\"\n",
        "    Compute the perplexity and the P-row for a specific value of the\n",
        "    precision of a Gaussian distribution.\n",
        "    \n",
        "    As we see in the paper, it can be interpreted as a smooth measure of the \n",
        "    effective number of neighbors (non-integer).\n",
        "    \"\"\"\n",
        "    # TASK: Follow equation 1 and compute the numerators of all the p_j|i.\n",
        "    \n",
        "    # TODO: Assuming beta represents whatever variance division \n",
        "    # term the algorithm decides, multiply and exponentiate to get P's \n",
        "    # numerator. Check your signs!\n",
        "    # Hint: No subtraction is necessary because x_i's position is \n",
        "    # treated as 0, since we are looking at the Gaussian around x_i.\n",
        "    \n",
        "    P = ...\n",
        "        \n",
        "    # computing perplexity in a vectorized + fast manner\n",
        "    H = np.log(np.sum(P)) + beta * np.sum(D * P) / np.sum(P)\n",
        "    \n",
        "    # TODO: Normalize P to apply conditioning (1 line)\n",
        "    P = ...\n",
        "        \n",
        "    return H, P"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaYOkb_4Bn4i",
        "colab_type": "text"
      },
      "source": [
        "Now we will use the perplexity and conditional probability distributions we have computed, and brute force search for conditional Gaussians that have the same perplexity! (i.e. the $P_i$'s, or the rows of the $n$ by $n$ $P$ matrix)\n",
        "\n",
        "A way to intuitively think about this is that we should expect any two points to estimate that the \"soft\" number of neighbors is the same, if they're in the same neighborhood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CEfk-qkI9xCn",
        "colab": {}
      },
      "source": [
        "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
        "    \"\"\"\n",
        "    Performs a binary search to get P-values in such a way that each\n",
        "    conditional Gaussian has the same perplexity.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize some variables\n",
        "    print(\"Computing pairwise distances...\")\n",
        "    (n, d) = X.shape\n",
        "    sum_X = np.sum(np.square(X), 1)\n",
        "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
        "\n",
        "    # NOTE: the P you compute should be of this shape. \n",
        "    # Make sure you understand why!\n",
        "    P = np.zeros((n, n))\n",
        "    beta = np.ones((n, 1))\n",
        "    \n",
        "    # NOTE: desired perplexity\n",
        "    logU = np.log(perplexity)\n",
        "\n",
        "    # Loop over all datapoints\n",
        "    for i in range(n):\n",
        "\n",
        "        # Print progress\n",
        "        if i % 500 == 0:\n",
        "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
        "\n",
        "        # Compute the Gaussian kernel and entropy for the current precision\n",
        "        betamin = -np.inf\n",
        "        betamax = np.inf\n",
        "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
        "        \n",
        "        # TODO: Utilize your newly written function to get perplexity and Pvals \n",
        "        # (1 line)\n",
        "        H, thisP = ...\n",
        "\n",
        "        # TODO: Evaluate whether the perplexity is within tolerance (1 line)\n",
        "        \n",
        "        Hdiff = ... \n",
        "        # Hint - don't apply absolute value yet - we need to know the sign\n",
        "        # of this value for binary search! You can apply abs in the loop cond.\n",
        "\n",
        "        tries = 0\n",
        "        \n",
        "        # TODO:\n",
        "        # Write a loop condition that continues if we're both below the max\n",
        "        # number of tries (50), and the absolute difference is greater than \n",
        "        # the tolerance.\n",
        "\n",
        "        while ... :\n",
        "\n",
        "            # TASK: If not, increase or decrease precision\n",
        "            # What follows here is a version of the standard binary search checks.\n",
        "            # The idea is that if we're too high (Hdiff positive), we can set the \n",
        "            # new min to be our current beta, and move our beta towards the max.\n",
        "            # If the max is currently infinity, we should just double beta.\n",
        "            # If the max is an actual number, set beta to be the avg of it and max.\n",
        "\n",
        "            if Hdiff > 0:\n",
        "                betamin = beta[i].copy()\n",
        "                if betamax == np.inf or betamax == -np.inf:\n",
        "                    # adjust the beta\n",
        "                    # TODO: \n",
        "                    beta[i] = ...\n",
        "                else:\n",
        "                    # adjust the beta\n",
        "                    # TODO: \n",
        "                    beta[i] = ...\n",
        "            else:\n",
        "                # Opposite of above (if Hdiff is negative)\n",
        "                betamax = beta[i].copy()\n",
        "                if betamin == np.inf or betamin == -np.inf:\n",
        "                    # TODO: \n",
        "                    beta[i] = ...\n",
        "                else:\n",
        "                    # TODO: \n",
        "                    beta[i] = ...\n",
        "\n",
        "            # TODO: Recompute the values again using your function and the new betas\n",
        "            H, thisP = ...\n",
        "            \n",
        "            # TODO: Recompute Hdiff\n",
        "            # (same as \"Evaluate whether the perplexity is within tolerance\")\n",
        "            Hdiff = ...\n",
        "            \n",
        "            tries += 1\n",
        "\n",
        "        # Set the final row of P\n",
        "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
        "\n",
        "    # Return final P-matrix\n",
        "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
        "    return P"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONrq9ewPBn4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pca(X=np.array([]), no_dims=50):\n",
        "    \"\"\"\n",
        "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
        "        no_dims dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Preprocessing the data using PCA...\")\n",
        "    (n, d) = X.shape\n",
        "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
        "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
        "    Y = np.dot(X, M[:, 0:no_dims])\n",
        "    return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqQ2zUuaBn4o",
        "colab_type": "text"
      },
      "source": [
        "Now, we will use the Student t-distribution with one degree of freedom to compute Q, the matrix of joint probabiltiies in low dimensions (see equation 4 of the paper).\n",
        "\n",
        "By the definition of norm squared (as we have discussed in previous assignments, the numerator can be expanded out as $$(1 + y_i^{T} y_i - 2y_i^T y_j + y_j^T y_j)^{-1}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVYpbhB_Bn4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
        "    \"\"\"\n",
        "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
        "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
        "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check inputs\n",
        "    if isinstance(no_dims, float):\n",
        "        print(\"Error: array X should have type float.\")\n",
        "        return -1\n",
        "    if round(no_dims) != no_dims:\n",
        "        print(\"Error: number of dimensions should be an integer.\")\n",
        "        return -1\n",
        "\n",
        "    # Initialize variables\n",
        "    X = pca(X, initial_dims).real\n",
        "    (n, d) = X.shape\n",
        "    max_iter = 400\n",
        "    initial_momentum = 0.5\n",
        "    final_momentum = 0.8\n",
        "    eta = 500\n",
        "    min_gain = 0.01\n",
        "    Y = np.random.randn(n, no_dims)\n",
        "    dY = np.zeros((n, no_dims))\n",
        "    iY = np.zeros((n, no_dims))\n",
        "    gains = np.ones((n, no_dims))\n",
        "\n",
        "    # Compute P-values\n",
        "    P = x2p(X, 1e-5, perplexity)\n",
        "    \n",
        "    P = P + np.transpose(P)\n",
        "    P = P / np.sum(P)\n",
        "    P = P * 4 # early exaggeration\n",
        "    P = np.maximum(P, 1e-12)\n",
        "\n",
        "    # Run iterations\n",
        "    for iter in range(max_iter):\n",
        "\n",
        "        # TASK: Compute pairwise affinities (Q)\n",
        "        \n",
        "        # SUBTASK: compute the numerator `num`\n",
        "        \n",
        "        # TODO: compute the first terms (all of the y_i's dotted with themselves)\n",
        "        \n",
        "        first term = ...\n",
        "        # HINT: first use np.square to square everything. Think about which way\n",
        "        # a \"y_i\" goes - is it a row or a column?\n",
        "        \n",
        "        # TODO: Compute the middle term (the negative -2 one)\n",
        "        # to do so, compute the *outer product* of Y with itself (use np.dot)\n",
        "        \n",
        "        middle_term = ...\n",
        "        # HINT: Why do we use the outer product here? What's the shape of middle_term?\n",
        "        \n",
        "        # apply the plus 1 and inverse on your previous value:\n",
        "        norm_sq = np.add(np.add(middle_term, first_term).T, first_term)\n",
        "        num = 1. / (1. + norm_sq)\n",
        "        \n",
        "        # END OF SUBTASK\n",
        "        \n",
        "        # TODO: set the diagonal of numerator to 0, and normalize it to get Q\n",
        "        # (2 lines)\n",
        "        ...\n",
        "        Q = ...\n",
        "        \n",
        "        # END OF TASK\n",
        "        \n",
        "        # this makes Q nonzero/non-negative, because floats are terrible\n",
        "        Q = np.maximum(Q, 1e-12)\n",
        "\n",
        "        # Compute gradient\n",
        "        PQ = P - Q\n",
        "        for i in range(n):\n",
        "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
        "\n",
        "        # Perform the update\n",
        "        if iter < 20:\n",
        "            momentum = initial_momentum\n",
        "        else:\n",
        "            momentum = final_momentum\n",
        "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
        "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
        "        gains[gains < min_gain] = min_gain\n",
        "        iY = momentum * iY - eta * (gains * dY)\n",
        "        Y = Y + iY\n",
        "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
        "\n",
        "        # Compute current value of cost function\n",
        "        if (iter + 1) % 10 == 0:\n",
        "            # TODO: Implement KL Divergence cost function on P and Q\n",
        "\n",
        "            C = ...\n",
        "            # HINT: make sure you wrap it with an np.sum so it's a scalar\n",
        "\n",
        "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
        "\n",
        "        # Stop exaggerating about P-values\n",
        "        if iter == 100:\n",
        "            P = P / 4.\n",
        "\n",
        "    # Return solution\n",
        "    return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHwu_wO5Bn4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run TSNE again to check that the implementation's output matches the library version's output!\n",
        "X_tsne = tsne(X)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(\n",
        "    x=X_tsne[:,0], y=X_tsne[:,1],\n",
        "    hue=y,\n",
        "    palette=sns.color_palette(\"hls\", 10),\n",
        "    legend=\"full\",\n",
        "    alpha=0.75\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeu3d7s7NdwM",
        "colab_type": "text"
      },
      "source": [
        "If your implementation is correct, you should see good separation much like the run in Question 1, though the shapes and locations of the clusters likely won't be the same (due to our implementation and parameters differing slightly from the sklearn t-SNE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLZ70x3KB6n",
        "colab_type": "text"
      },
      "source": [
        "## 3. Conceptual Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fh5QZ8_KIjG",
        "colab_type": "text"
      },
      "source": [
        "### 3.a) To optimize the performance of the method, we scaled P by 4 for the first 100 iterations of gradient descent. How does this improve performance?\n",
        "\n",
        "TODO: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7zVk9oGVbtD",
        "colab_type": "text"
      },
      "source": [
        "### 3.b) In the above implementation, we used PCA to first reduce the input to 50 dimensions/features - why is it preferred to run t-SNE on data that doesn't have a high number of dimensions?\n",
        "\n",
        "TODO: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDexfTq7O8QK",
        "colab_type": "text"
      },
      "source": [
        "### 3.c) t-SNE is known to be a 'nonlinear' dimensionality reduction method. As such the distance between points does not accurately reflect distance in the original space. Based on the math behind the technique, explain why t-SNE is nonlinear.\n",
        "\n",
        "TODO: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeCij84RYJTH",
        "colab_type": "text"
      },
      "source": [
        "### 3.d) Explain the improvement of this method over SNE - specifically, why is the distance between two points in the lower-dimensional space defined as (1) rather than the definition of distance used for the higher dimensional space (2), ignoring the symmetrization?\n",
        "\n",
        "\n",
        "$$q_{i j} = \\frac{(1+||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1+||y_k - y_l||^2)^{-1}} \\;\\;\\;\\;\\;\\;  (1)$$\n",
        "\n",
        "$$p_{i j} = \\frac{\\text{exp}(-||x_i - x_j||^2 / 2\\sigma^2)}{\\sum_{k \\neq l}\\text{exp}(-||x_k - x_l||^2/ 2\\sigma^2)} \\;\\;\\;\\;\\;\\;  (2)$$\n",
        "\n",
        "\n",
        "TODO: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-R2DUfwdNR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}