{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCztPzhn8wZp",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 5: Gradient Descent - Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-14RLZcH31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "\n",
        "diabetes_X, diabetes_y = sklearn.datasets.load_diabetes(return_X_y = True)\n",
        "split = sklearn.model_selection.train_test_split(diabetes_X, diabetes_y)\n",
        "diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvHU9OiTcLgt",
        "colab_type": "text"
      },
      "source": [
        "## 1. Loss Functions\n",
        "\n",
        "In this exercise we'll be considering a simple linear model:\n",
        "$$y \\approx \\theta x$$\n",
        "The hypothesis for the model is written as\n",
        "$$h(\\theta) = \\theta x$$\n",
        "\n",
        "### a. Fill in the following methods for the loss functions and their derivatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z1kDe-8fwbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squared_loss(h, y):\n",
        "    \"\"\"\n",
        "    Returns the squared difference of each row, averaged over all rows (each datapoint)\n",
        "    Input:\n",
        "    h: n by 1 vector\n",
        "    y: n by 1 vector\n",
        "    Output:\n",
        "    loss: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    loss = None\n",
        "    loss = np.mean(np.square(np.subtract(h,y)))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vruF1dde7NLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squared_deriv(h, y):\n",
        "    \"\"\"\n",
        "    Returns the gradient wrt theta of the squared loss, averaged over the datapoints\n",
        "    Input:\n",
        "    h: n by 1 vector\n",
        "    y: n by 1 vector\n",
        "    Output:\n",
        "    grad: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    grad = None\n",
        "    \n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jFOdAa0gYyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def abs_loss(h, y):\n",
        "    \"\"\"\n",
        "    Returns the absolute value of the difference of each row, averaged over the datapoints\n",
        "    Input:\n",
        "    h: n by 1 vector\n",
        "    y: n by 1 vector\n",
        "    Output:\n",
        "    loss: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    loss = None\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saxUgAYrgxLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def abs_deriv(h, y):\n",
        "    \"\"\"\n",
        "    Returns the gradient wrt theta of the absolute loss, averaged over the datapoints\n",
        "    Input:\n",
        "    h: n by 1 vector\n",
        "    y: n by 1 vector\n",
        "    Output:\n",
        "    grad: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    grad = None\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXXkxnha8sDf",
        "colab_type": "text"
      },
      "source": [
        "### b. Plot the loss and the gradient for the provided data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsuoZEwM8q3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data you'll use with the above methods\n",
        "simple_x = np.arange(-20,20,0.5)\n",
        "\n",
        "# yields a float between 3 and 7\n",
        "true_theta = 4*np.random.random_sample()+3\n",
        "simple_y = true_theta*simple_x + np.random.normal(scale = 10, size=simple_x.shape)\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.scatter(simple_x, simple_y, linewidths=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag-8NaC2ALHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible theta values (to iterate through)\n",
        "pos_theta = np.arange(0, 10, 0.1)\n",
        "\n",
        "# For convenience in using the above methods\n",
        "simple_x.reshape((simple_x.shape[0], 1));\n",
        "simple_y.reshape((simple_y.shape[0], 1)); "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq3YBLBGFSaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: plot squared loss and gradient\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.suptitle(\"Squared Loss Function\")\n",
        "plt.subplots_adjust(hspace=0.2)\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# TODO:\n",
        "\n",
        "plt.title(\"Loss\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "# TODO:\n",
        "\n",
        "plt.title(\"Gradient\")\n",
        "plt.xlabel(\"Theta\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOSVrgfSSKY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: plot absolute loss and gradient\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.suptitle(\"Absolute Loss Function\")\n",
        "plt.subplots_adjust(hspace=0.2)\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# TODO:\n",
        "\n",
        "plt.title(\"Loss\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "# TODO:\n",
        "\n",
        "plt.title(\"Gradient\")\n",
        "plt.xlabel(\"Theta\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTffZoO2SQkF",
        "colab_type": "text"
      },
      "source": [
        "### c. Given that the gradient descent algorithm uses the first derivative to  find a local minimum, which of the above loss functions is preferable for linear regression using gradient descent? Briefly explain using the above plots.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRa1zrwFT4EU",
        "colab_type": "text"
      },
      "source": [
        "## 2. Gradient Descent Linear Regression\n",
        "\n",
        "Here you'll implement a linear regressor using gradient descent and the diabetes dataset initialized at the top of this assignment. Using the loss function you chose for 1c, the gradient descent algorithm will follow the below given formula to update the parameters and find the optimal solution.\n",
        "\n",
        "The model:\n",
        "$$ y \\approx X w $$\n",
        "Hypothesis:\n",
        "$$ h(w) = Xw $$\n",
        "\n",
        "Hint: w is a vector, thus the gradient will also need to be a vector.\n",
        "\n",
        "**Gradient Descent Update Function:**\n",
        "\n",
        "$$w_{n+1} = w_n - \\alpha \\nabla L(w_n) $$\n",
        "\n",
        "Due to the relatively small size of the dataset, use all datapoints for computing the gradient (also known as batch gradient descent - compare to stochastic gradient descent, an optimization over batch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALoDcIeJSv3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gd_linreg(X, y, alpha, epsilon=0.001):\n",
        "    \"\"\"\n",
        "    Performs linear regression on X and y using gradient descent\n",
        "\n",
        "    Input:\n",
        "    X: n x m matrix - n datapoints, m features\n",
        "    y: n x 1 vector\n",
        "    alpha: step size for gradient descent update\n",
        "    epsilon: maximum difference between the w_n+1 and w_n for convergence \n",
        "\n",
        "    Output:\n",
        "    w: m x 1 vector - weights for each feature of a data point\n",
        "    losses: array of losses at each step/iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    w = None\n",
        "    losses = None\n",
        "\n",
        "    return w, losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbtnhZq2sOWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: set an appropriate alpha\n",
        "alpha = None\n",
        "\n",
        "diabetes_w, losses = gd_linreg(diabetes_X_train, diabetes_y_train, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77vjAP4snO4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot losses (may help find a good value for alpha)\n",
        "num_iter = len(losses)\n",
        "\n",
        "plt.title(\"Loss over Iterations of Gradient Descent\")\n",
        "plt.plot(range(1, num_iter+1), losses, c = 'orange')\n",
        "plt.plot(range(1,num_iter+1), 26226.66*np.ones(num_iter), c = 'blue');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE5D5T_LpINE",
        "colab_type": "text"
      },
      "source": [
        "## 3. Evaluate your Implementation\n",
        "\n",
        "### a. Find your chosen loss using the output weights as found above with the test set using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKspdYBRtmLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:\n",
        "gd_test_loss = None\n",
        "gd_test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUGxorrEtmnv",
        "colab_type": "text"
      },
      "source": [
        "### b. Using sklearn's OLS Linear Regression method, evaluate the loss of the out of the box method.\n",
        "\n",
        "Steps: Fit the regressor using the training data (X_train and y_train) then predict using X_test, then use your chosen loss method to evaluate the output against y_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-R9RbYMpOyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "regr = linear_model.LinearRegression(fit_intercept=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f19pE0_UvB17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit using train and predict for test\n",
        "# TODO:\n",
        "\n",
        "linreg_h_w = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y1CUj9btGVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:\n",
        "linreg_test_loss = None\n",
        "linreg_test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}