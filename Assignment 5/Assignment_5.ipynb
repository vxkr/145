{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCztPzhn8wZp",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 5: Gradient Descent - Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-14RLZcH31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets\n",
        "import sklearn.model_selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcsj0iecTRWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up data \n",
        "diabetes_X, diabetes_y = sklearn.datasets.load_diabetes(return_X_y = True)\n",
        "# Split into train and test sets\n",
        "split = sklearn.model_selection.train_test_split(diabetes_X, diabetes_y)\n",
        "diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvHU9OiTcLgt",
        "colab_type": "text"
      },
      "source": [
        "## 1. Loss Functions\n",
        "\n",
        "In this exercise we'll be considering a simple linear model:\n",
        "$$y \\approx \\theta x$$\n",
        "The hypothesis for the model is written as\n",
        "$$h(\\theta) = \\theta x$$\n",
        "\n",
        "### a. Fill in the following methods for the loss functions and their derivatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z1kDe-8fwbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squared_loss(X, theta, y):\n",
        "    \"\"\"\n",
        "    Returns the squared loss\n",
        "    \n",
        "    Input:\n",
        "    X: n length vector - n datapoints\n",
        "    theta: scalar\n",
        "    y: n length vector\n",
        "    \n",
        "    Output:\n",
        "    loss: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    loss = None\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vruF1dde7NLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squared_deriv(X, theta, y):\n",
        "    \"\"\"\n",
        "    Returns the gradient wrt theta of the squared loss\n",
        "    \n",
        "    Input:\n",
        "    X: n length vector - n datapoints\n",
        "    theta: scalar\n",
        "    y: n length vector\n",
        "    \n",
        "    Output:\n",
        "    grad: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    grad = None\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jFOdAa0gYyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def abs_loss(X, theta, y):\n",
        "    \"\"\"\n",
        "    Returns the absolute value loss\n",
        "    \n",
        "    Input:\n",
        "    X: n length vector - n datapoints\n",
        "    theta: scalar\n",
        "    y: n length vector\n",
        "    \n",
        "    Output:\n",
        "    loss: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    loss = None\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saxUgAYrgxLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def abs_deriv(X, theta, y):\n",
        "    \"\"\"\n",
        "    Returns the gradient wrt theta of the absolute loss\n",
        "    \n",
        "    Input:\n",
        "    X: n length vector - n datapoints\n",
        "    theta: scalar\n",
        "    y: n length vector\n",
        "    \n",
        "    Output:\n",
        "    grad: scalar\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    grad = None\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXXkxnha8sDf",
        "colab_type": "text"
      },
      "source": [
        "### b. Plot the loss and the gradient for the provided data\n",
        "\n",
        "In other words, compute an array of losses + gradients with pos_theta (find  loss and gradient for each possible theta)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsuoZEwM8q3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data you'll use with the above methods\n",
        "simple_x = np.arange(-20,20,0.5)\n",
        "\n",
        "# Yields a float between 3 and 7\n",
        "true_theta = 4*np.random.random_sample()+3\n",
        "# Add noise and scale y\n",
        "simple_y = true_theta*simple_x + np.random.normal(scale = 10, size=simple_x.shape)\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.title(\"Simple Data for Linear Regression\")\n",
        "plt.scatter(simple_x, simple_y, linewidths=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag-8NaC2ALHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible theta values (to iterate through)\n",
        "pos_theta = np.arange(0, 10, 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq3YBLBGFSaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot squared loss and gradient\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.suptitle(\"Squared Loss Function\")\n",
        "plt.subplots_adjust(hspace=0.2)\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# TODO: Find and plot loss\n",
        "\n",
        "plt.title(\"Loss\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "# TODO: Find and plot gradient\n",
        "\n",
        "plt.plot(pos_theta, np.zeros_like(pos_theta))\n",
        "plt.title(\"Gradient\")\n",
        "plt.xlabel(\"Theta\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOSVrgfSSKY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot absolute loss and gradient\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.suptitle(\"Absolute Loss Function\")\n",
        "plt.subplots_adjust(hspace=0.2)\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "# TODO: Find and plot loss\n",
        "\n",
        "plt.title(\"Loss\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "# TODO: Find and plot gradient\n",
        "\n",
        "plt.plot(pos_theta, np.zeros_like(pos_theta))\n",
        "plt.title(\"Gradient\")\n",
        "plt.xlabel(\"Theta\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTffZoO2SQkF",
        "colab_type": "text"
      },
      "source": [
        "### c. Given that the gradient descent algorithm uses the first derivative to  find a local minimum, which of the above loss functions is preferable for linear regression using gradient descent? Briefly explain using the above plots.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRa1zrwFT4EU",
        "colab_type": "text"
      },
      "source": [
        "## 2. Gradient Descent Linear Regression\n",
        "\n",
        "Here you'll implement a linear regressor using gradient descent and the diabetes dataset initialized at the top of this assignment. Using the L2-norm squared loss function, the gradient descent algorithm will follow the below given formula to update the parameters and find the optimal solution.\n",
        "______\n",
        "The model:\n",
        "$$ y \\approx X w $$\n",
        "Hypothesis:\n",
        "$$ h(w) = Xw $$\n",
        "______\n",
        "**Gradient Descent Update Function:**\n",
        "\n",
        "$$ w_{n+1} = w_n - \\alpha \\nabla L(w_n) $$\n",
        "\n",
        "Due to the relatively small size of the dataset, use all datapoints for computing the gradient (also known as batch gradient descent - compare to stochastic gradient descent, an optimization over batch).\n",
        "\n",
        "_____\n",
        "\n",
        "The L2-norm squared loss for this model is written as\n",
        "$$ L(w) = ||X w - y ||^2_2 $$\n",
        "\n",
        "_____\n",
        "\n",
        "### a. Find the gradient of the loss function with respect to w.\n",
        "\n",
        "Answer:\n",
        "\n",
        "TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URs8LQqpkKN7",
        "colab_type": "text"
      },
      "source": [
        "### b. Implement the following methods to perform linear regression using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALoDcIeJSv3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gd_linreg(X, y, alpha, loss_func, derivative_func, epsilon=0.001, max_iters=10000):\n",
        "    \"\"\"\n",
        "    Performs linear regression on X and y using gradient descent\n",
        "\n",
        "    Input:\n",
        "    X: n x m matrix - n datapoints, m features\n",
        "    y: n length vector\n",
        "    alpha: step size for gradient descent update\n",
        "    loss_func: method to compute loss between two quantities\n",
        "    derivative_func: method to compute gradient wrt w\n",
        "    epsilon: maximum difference between the w_n+1 and w_n for convergence \n",
        "\n",
        "    Output:\n",
        "    w: m length vector - weights for each feature of a data point\n",
        "    losses: array of losses at each step/iteration\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    w = None\n",
        "    losses = None\n",
        "\n",
        "    return w, losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mouSJhk3Mft0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_linreg(X, w, y):\n",
        "    \"\"\"\n",
        "    Evaluates the loss function\n",
        "    \n",
        "    Input:\n",
        "    X: n x m - n datapoints, m features\n",
        "    w: m length vector - weights for features in X\n",
        "    y: n length vector\n",
        "    \n",
        "    Output:\n",
        "    loss: scalar\n",
        "    \"\"\"\n",
        "    #TODO:\n",
        "    loss = None\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-nBBibeKjKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivative_loss_linreg(X, w, y):\n",
        "    \"\"\"\n",
        "    Finds the derivative of the loss function wrt w\n",
        "    \n",
        "    Input:\n",
        "    X: n x m - n datapoints, m features\n",
        "    w: m length vector - weights for features in X\n",
        "    y: n length vector\n",
        "\n",
        "    Output:\n",
        "    gradient: length m array - gradient wrt w\n",
        "    \"\"\"\n",
        "    #TODO:\n",
        "    grad = None\n",
        "\n",
        "    return grad\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCRPxQ4Rj9yU",
        "colab_type": "text"
      },
      "source": [
        "### c. Run gradient descent with an appropriate step size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbtnhZq2sOWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: set an appropriate alpha\n",
        "alpha = None\n",
        "\n",
        "diabetes_w, losses = gd_linreg(diabetes_X_train, diabetes_y_train, alpha, loss_linreg, derivative_loss_linreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77vjAP4snO4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot losses (may help find a good value for alpha)\n",
        "num_iter = len(losses)\n",
        "\n",
        "plt.title(\"Loss over Iterations of Gradient Descent\")\n",
        "plt.plot(range(1, num_iter+1), losses, c = 'orange');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE5D5T_LpINE",
        "colab_type": "text"
      },
      "source": [
        "## 3. Evaluate your Implementation\n",
        "\n",
        "### a. Find the loss for the training set and the test set using the weights found with gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKspdYBRtmLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:\n",
        "gd_train_loss = None\n",
        "gd_test_loss = None\n",
        "\n",
        "print(\"Method: Gradient Descent\")\n",
        "print(\"Training Loss: \" + str(gd_train_loss))\n",
        "print(\"Test Loss: \" + str(gd_test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUGxorrEtmnv",
        "colab_type": "text"
      },
      "source": [
        "### b. Write and implement the OLS solution for w.\n",
        "\n",
        "The OLS solution sets the above found gradient of the loss wrt to w to 0 (from 2a) and solves for w.\n",
        "\n",
        "Answer:\n",
        "\n",
        "TODO:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQX71SemiQiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OLS(X, y):\n",
        "    \"\"\"\n",
        "    Finds OLS solution to linear regression of X and y\n",
        "\n",
        "    Input:\n",
        "    X: n x m - n datapoints, m features\n",
        "    y: n length vector\n",
        "\n",
        "    Output:\n",
        "    w: m length vector - weights for features in X\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    w = None\n",
        "\n",
        "    return w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getJfguQZ9co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ols_w = OLS(diabetes_X_train, diabetes_y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_MW7BW0jnf0",
        "colab_type": "text"
      },
      "source": [
        "### c. Find the loss for the training set and the test set using the weights found with OLS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cOo2QbJbojD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:\n",
        "ols_train_loss = None\n",
        "ols_test_loss = None\n",
        "\n",
        "print(\"Method: OLS\")\n",
        "print(\"Training Loss: \" + str(ols_train_loss))\n",
        "print(\"Test Loss: \" + str(ols_test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}