{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCztPzhn8wZp"
   },
   "source": [
    "# Assignment 5: Gradient Descent - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nc-14RLZcH31"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gcsj0iecTRWP"
   },
   "outputs": [],
   "source": [
    "# Set up data \n",
    "diabetes_X, diabetes_y = sklearn.datasets.load_diabetes(return_X_y = True)\n",
    "# Split into train and test sets\n",
    "split = sklearn.model_selection.train_test_split(diabetes_X, diabetes_y)\n",
    "diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvHU9OiTcLgt"
   },
   "source": [
    "## 1. Loss Functions\n",
    "\n",
    "In this exercise we'll be considering a simple linear model:\n",
    "\n",
    "$$y \\approx \\theta x$$\n",
    "\n",
    "The hypothesis for the model is written as\n",
    "\n",
    "$$h(\\theta) = \\theta x$$\n",
    "\n",
    "### a. Fill in the following methods for the loss functions and their derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8z1kDe-8fwbg"
   },
   "outputs": [],
   "source": [
    "def squared_loss(X, theta, y):\n",
    "    \"\"\"\n",
    "    Returns the squared loss\n",
    "    \n",
    "    Input:\n",
    "    X: n length vector - n datapoints\n",
    "    theta: scalar\n",
    "    y: n length vector\n",
    "    \n",
    "    Output:\n",
    "    loss: scalar\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    loss = None\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vruF1dde7NLh"
   },
   "outputs": [],
   "source": [
    "def squared_deriv(X, theta, y):\n",
    "    \"\"\"\n",
    "    Returns the gradient wrt theta of the squared loss\n",
    "    \n",
    "    Input:\n",
    "    X: n length vector - n datapoints\n",
    "    theta: scalar\n",
    "    y: n length vector\n",
    "    \n",
    "    Output:\n",
    "    grad: scalar\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    grad = None\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jFOdAa0gYyx"
   },
   "outputs": [],
   "source": [
    "def abs_loss(X, theta, y):\n",
    "    \"\"\"\n",
    "    Returns the absolute value loss\n",
    "    \n",
    "    Input:\n",
    "    X: n length vector - n datapoints\n",
    "    theta: scalar\n",
    "    y: n length vector\n",
    "    \n",
    "    Output:\n",
    "    loss: scalar\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    loss = None\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saxUgAYrgxLV"
   },
   "outputs": [],
   "source": [
    "def abs_deriv(X, theta, y):\n",
    "    \"\"\"\n",
    "    Returns the gradient wrt theta of the absolute loss\n",
    "    \n",
    "    Input:\n",
    "    X: n length vector - n datapoints\n",
    "    theta: scalar\n",
    "    y: n length vector\n",
    "    \n",
    "    Output:\n",
    "    grad: scalar\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    grad = None\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXXkxnha8sDf"
   },
   "source": [
    "### b. Plot the loss and the gradient for the provided data, simple_x and simple_y\n",
    "\n",
    "In other words, compute an array of losses + gradients with pos_theta (find  loss and gradient for each possible theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsuoZEwM8q3d"
   },
   "outputs": [],
   "source": [
    "# Data you'll use with the above methods\n",
    "simple_x = np.arange(-20,20,0.5)\n",
    "\n",
    "# Yields a float between 3 and 7\n",
    "true_theta = 4*np.random.random_sample()+3\n",
    "# Add noise and scale y\n",
    "simple_y = true_theta*simple_x + np.random.normal(scale = 10, size=simple_x.shape)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.title(\"Simple Data for Linear Regression\")\n",
    "plt.scatter(simple_x, simple_y, linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag-8NaC2ALHK"
   },
   "outputs": [],
   "source": [
    "# Possible theta values (to iterate through)\n",
    "pos_theta = np.arange(0, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jq3YBLBGFSaM"
   },
   "outputs": [],
   "source": [
    "# Plot squared loss and gradient\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.suptitle(\"Squared Loss Function\")\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "# TODO: Find and plot loss\n",
    "\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# TODO: Find and plot gradient\n",
    "\n",
    "plt.plot(pos_theta, np.zeros_like(pos_theta))\n",
    "plt.title(\"Gradient\")\n",
    "plt.xlabel(\"Theta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOSVrgfSSKY6"
   },
   "outputs": [],
   "source": [
    "# Plot absolute loss and gradient\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.suptitle(\"Absolute Loss Function\")\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "# TODO: Find and plot loss\n",
    "\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# TODO: Find and plot gradient\n",
    "\n",
    "plt.plot(pos_theta, np.zeros_like(pos_theta))\n",
    "plt.title(\"Gradient\")\n",
    "plt.xlabel(\"Theta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTffZoO2SQkF"
   },
   "source": [
    "### c. Given that the gradient descent algorithm uses the first derivative to  find a local minimum, which of the above loss functions is preferable for linear regression using gradient descent? Briefly explain using the above plots.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRa1zrwFT4EU"
   },
   "source": [
    "## 2. Gradient Descent Linear Regression\n",
    "\n",
    "Here you'll implement a linear regressor using gradient descent and the diabetes dataset initialized at the top of this assignment. Using the L2-norm squared loss function, the gradient descent algorithm will follow the below given formula to update the parameters and find the optimal solution.\n",
    "______\n",
    "The model:\n",
    "\n",
    "$$ y \\approx X w $$\n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "$$ h(w) = Xw $$\n",
    "______\n",
    "**Gradient Descent Update Function:**\n",
    "\n",
    "$$ w_{n+1} = w_n - \\alpha \\nabla L(w_n) $$\n",
    "\n",
    "Due to the relatively small size of the dataset, use all datapoints for computing the gradient (also known as batch gradient descent - compare to stochastic gradient descent, an optimization over batch).\n",
    "\n",
    "_____\n",
    "\n",
    "The L2-norm squared loss for this model is written as\n",
    "\n",
    "$$ L(w) = ||X w - y ||^2_2 $$\n",
    "\n",
    "_____\n",
    "\n",
    "### a. Find the gradient of the loss function with respect to w.\n",
    "\n",
    "Answer:\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URs8LQqpkKN7"
   },
   "source": [
    "### b. Implement the following methods to perform linear regression using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALoDcIeJSv3y"
   },
   "outputs": [],
   "source": [
    "def gd_linreg(X, y, alpha, loss_func, derivative_func, epsilon=0.001, max_iters=10000):\n",
    "    \"\"\"\n",
    "    Performs linear regression on X and y using gradient descent\n",
    "\n",
    "    Input:\n",
    "    X: n x m matrix - n datapoints, m features\n",
    "    y: n length vector\n",
    "    alpha: step size for gradient descent update\n",
    "    loss_func: method to compute loss between two quantities\n",
    "    derivative_func: method to compute gradient wrt w\n",
    "    epsilon: maximum difference between the w_n+1 and w_n for convergence \n",
    "\n",
    "    Output:\n",
    "    w: m length vector - weights for each feature of a data point\n",
    "    losses: array of losses at each step/iteration\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    w = None\n",
    "    losses = None\n",
    "\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mouSJhk3Mft0"
   },
   "outputs": [],
   "source": [
    "def loss_linreg(X, w, y):\n",
    "    \"\"\"\n",
    "    Evaluates the loss function\n",
    "    \n",
    "    Input:\n",
    "    X: n x m - n datapoints, m features\n",
    "    w: m length vector - weights for features in X\n",
    "    y: n length vector\n",
    "    \n",
    "    Output:\n",
    "    loss: scalar\n",
    "    \"\"\"\n",
    "    #TODO:\n",
    "    loss = None\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-nBBibeKjKl"
   },
   "outputs": [],
   "source": [
    "def derivative_loss_linreg(X, w, y):\n",
    "    \"\"\"\n",
    "    Finds the derivative of the loss function wrt w\n",
    "    \n",
    "    Input:\n",
    "    X: n x m - n datapoints, m features\n",
    "    w: m length vector - weights for features in X\n",
    "    y: n length vector\n",
    "\n",
    "    Output:\n",
    "    gradient: length m array - gradient wrt w\n",
    "    \"\"\"\n",
    "    #TODO:\n",
    "    grad = None\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCRPxQ4Rj9yU"
   },
   "source": [
    "### c. Run gradient descent with an appropriate step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbtnhZq2sOWi"
   },
   "outputs": [],
   "source": [
    "# TODO: set an appropriate alpha\n",
    "alpha = None\n",
    "\n",
    "diabetes_w, losses = gd_linreg(diabetes_X_train, diabetes_y_train, alpha, loss_linreg, derivative_loss_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77vjAP4snO4q"
   },
   "outputs": [],
   "source": [
    "# Plot losses (may help find a good value for alpha)\n",
    "num_iter = len(losses)\n",
    "\n",
    "plt.title(\"Loss over Iterations of Gradient Descent\")\n",
    "plt.plot(range(1, num_iter+1), losses, c = 'orange');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XE5D5T_LpINE"
   },
   "source": [
    "## 3. Evaluate your Implementation\n",
    "\n",
    "### a. Find the loss for the training set and the test set using the weights found with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKspdYBRtmLI"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "gd_train_loss = None\n",
    "gd_test_loss = None\n",
    "\n",
    "print(\"Method: Gradient Descent\")\n",
    "print(\"Training Loss: \" + str(gd_train_loss))\n",
    "print(\"Test Loss: \" + str(gd_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUGxorrEtmnv"
   },
   "source": [
    "### b. Write and implement the OLS solution for w.\n",
    "\n",
    "The OLS solution sets the above found gradient of the loss wrt to w to 0 (from 2a) and solves for w.\n",
    "\n",
    "Answer:\n",
    "\n",
    "TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQX71SemiQiA"
   },
   "outputs": [],
   "source": [
    "def OLS(X, y):\n",
    "    \"\"\"\n",
    "    Finds OLS solution to linear regression of X and y\n",
    "\n",
    "    Input:\n",
    "    X: n x m - n datapoints, m features\n",
    "    y: n length vector\n",
    "\n",
    "    Output:\n",
    "    w: m length vector - weights for features in X\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO:\n",
    "    w = None\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "getJfguQZ9co"
   },
   "outputs": [],
   "source": [
    "ols_w = OLS(diabetes_X_train, diabetes_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_MW7BW0jnf0"
   },
   "source": [
    "### c. Find the loss for the training set and the test set using the weights found with OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cOo2QbJbojD"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "ols_train_loss = None\n",
    "ols_test_loss = None\n",
    "\n",
    "print(\"Method: OLS\")\n",
    "print(\"Training Loss: \" + str(ols_train_loss))\n",
    "print(\"Test Loss: \" + str(ols_test_loss))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
